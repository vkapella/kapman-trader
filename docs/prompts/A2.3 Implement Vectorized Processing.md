YYou are implementing KapMan story A2.3: “Vectorized Chunk Execution”.

Context:
- Current A2 metric computation is chunked and parallelized but still computes indicators ticker-by-ticker.
- This causes heavy Python loop overhead.
- We want to vectorize indicator computation per chunk while preserving all semantics.

Goals:
- Eliminate inner per-ticker indicator loops.
- Compute indicators once per chunk using wide pandas DataFrames.
- Extract only the final row per ticker for persistence.
- Preserve determinism, chunking, multiprocessing, logging, and DB write behavior.

Hard constraints:
1. Load OHLCV once per chunk into a wide DataFrame.
2. For each indicator category (momentum, trend, volatility, volume, others):
   - Compute indicator vectors once for the entire chunk.
3. Extract the most recent value per indicator per ticker.
4. Preserve deterministic ordering (tickers, JSON keys).
5. Preserve NULL semantics exactly (insufficient history or errors → NULL).
6. No schema changes.
7. No changes to multiprocessing, chunking, or DB upsert logic.

Implementation guidance:
- Refactor core/metrics/a2_local_ta_job.py.
- Replace per-ticker loops with vectorized category-level computation.
- Keep ta_indicator_surface.py as the authoritative indicator registry.
- Ensure vectorized paths use the same indicator definitions as before.
- Add timing stats to confirm chunk-level speedup.

Acceptance:
- Output JSON must match A2.2 exactly (byte-for-byte for same inputs).
- Chunk runtime must improve by at least 30%.
- All existing unit and integration tests must pass unchanged.

Do NOT:
- Add distributed execution.
- Change indicator definitions.
- Reduce indicator surface.
- Change CLI flags or DB schema.

Deliverables:
- Updated implementation
- Updated timing instrumentation (if needed)
- Passing test suite